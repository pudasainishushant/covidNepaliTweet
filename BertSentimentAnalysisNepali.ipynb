{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce0c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n",
    "from scipy.spatial.distance import cosine \n",
    "import tokenizers \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import snowballstemmer\n",
    "import numpy\n",
    "import os \n",
    "import re\n",
    "import json\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98daed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_path = os.getcwd()+'/bert_model/vocab_low_data.txt'\n",
    "# model_path = os.getcwd()+'/blabla/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c99f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86375ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizers = BertTokenizer.from_pretrained(vocab_path)\n",
    "# model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path=model_path, return_dict = True, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b519b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"Shushant/nepaliBERT\", output_hidden_states = True, return_dict = True, output_attentions = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dc414c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = AutoTokenizer.from_pretrained(\"Shushant/nepaliBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd3f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(model, open('bert_embeddings','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ac250ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(tokenizers, open('bert_tokenizers','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b9e28",
   "metadata": {},
   "source": [
    "model = pickle.load(open('bert_embeddings','rb'))\n",
    "tokenizers= pickle.load(open('bert_tokenizers','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8b082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2963c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1871cd20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['क',\n",
       " 'मौ',\n",
       " '##ज',\n",
       " '##दा',\n",
       " 'लोक',\n",
       " '##तान',\n",
       " '##तर',\n",
       " '##िक',\n",
       " 'वय',\n",
       " '##वस',\n",
       " '##था',\n",
       " 'राज',\n",
       " '##य',\n",
       " 'पन',\n",
       " '##ः',\n",
       " '##सर',\n",
       " '##चना',\n",
       " '##सग',\n",
       " 'जोडिएका',\n",
       " 'हिजोका',\n",
       " 'सवाल',\n",
       " '##हर',\n",
       " '##लाई',\n",
       " 'यथा',\n",
       " '##स',\n",
       " '##थिति',\n",
       " '##मा',\n",
       " 'छोड',\n",
       " '##र',\n",
       " 'सबल',\n",
       " 'होला',\n",
       " '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.tokenize(\"के मौजुदा लोकतान्त्रिक व्यवस्था राज्य पुनःसंरचनासँग जोडिएका हिजोका सवालहरूलाई यथास्थितिमा छोडेर सबल होला?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'अनि तेस्रो चिन्ता मौसम परिवर्तनले हिमशिखरहरूमा परेका आघातसँगसँगै सिमानावारिपारि नदीले ल्याएका प्रकोपहरू कसरी सम्हाल्ने'\n",
    "# marked_text = \" [CLS] \"+text+\" [SEP] \"\n",
    "# tokenized_text = tokenizer.tokenize(marked_text)\n",
    "# indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "# tokens_tensors = torch.tensor([indexed_tokens])\n",
    "# segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a853e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     outputs = model(tokens_tensors, segments_tensors)\n",
    "#     hidden_states = outputs.hidden_states\n",
    "# #     print(hidden_states[-1])\n",
    "#     token_embeddings = hidden_states[-1]\n",
    "    \n",
    "#     token_embeddings = torch.squeeze(token_embeddings, dim = 0)\n",
    "    \n",
    "#     list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "#     print(list_token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nepali_stemmer = snowballstemmer.NepaliStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = ['तर','दुधमा तर बसेन|','तिम्रो घर आउन मन लाग्छ तर अल्छि लाग्छ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd86297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bert_text_preparation(text, tokenizer ):\n",
    "#     \"\"\"Preparing input for BERT\"\"\"\n",
    "    \n",
    "#     marked_text = \" [CLS] \" + text + \" [SEP] \"\n",
    "#     tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#     indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#     segments_ids = [1] * len(indexed_tokens) \n",
    "    \n",
    "#     # Convert inputs to Pytorch tensors\n",
    "#     tokens_tensors = torch.tensor([indexed_tokens])\n",
    "#     segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "#     return tokenized_text, tokens_tensors, segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a70ff12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "#     # Gradient claculation id disabled \n",
    "#     # Model is in inference mode\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(tokens_tensor, segments_tensors)\n",
    "#         # removing the first hidden state\n",
    "#         # the first state is the input state \n",
    "#         hidden_states = outputs.hidden_states\n",
    "    \n",
    "#     # Getting embeddings from final Bert Layer\n",
    "#     tokens_embeddings = hidden_states[-1]\n",
    "#     # Collasping the tensor into 1-dimension \n",
    "#     tokens_embeddings = torch.squeeze(tokens_embeddings, dim = 0)\n",
    "#     # Converting torchtensors to lists \n",
    "#     list_token_embeddings = [token_embed.tolist() for token_embed in tokens_embeddings]\n",
    "    \n",
    "#     return list_token_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d8c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "# target_word_embeddings = []\n",
    "\n",
    "# for text in texts:\n",
    "#     tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "#     list_token_embeddings = get_bert_embeddings(tokens_tensors, segments_tensors, model)\n",
    "#     ## list_token_embeddings has embeddings of the given words\n",
    "# #     word_index = tokenized_text.index('तर')\n",
    "#     word_embeddings = [list_token_embeddings[token] for token in tokenized_text]\n",
    "# #     word_embedding = list_token_embeddings[word_index]\n",
    "# #     print(word_embedding)\n",
    "# #     target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1afd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1caf3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d4db1a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# target_word_embeddings = []\n",
    "\n",
    "# for text in texts:\n",
    "#     tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizers)\n",
    "#     list_token_embeddings = get_bert_embeddings(tokens_tensors, segments_tensors, model)\n",
    "# #     print(len(list_token_embeddings))\n",
    "#     ## list_token_embeddings has embeddings of the given words\n",
    "#     word_index = tokenized_text.index('तर')\n",
    "#     word_embedding = list_token_embeddings[word_index]\n",
    "# #     print(word_embedding)\n",
    "#     target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c79f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word_embeddings[0] == target_word_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb28025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0578cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(target_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5144fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_distances = []\n",
    "# for text1, embed1 in zip(texts, target_word_embeddings):\n",
    "#     for text2, embed2 in zip(texts, target_word_embeddings):\n",
    "#         cos_dist = 1 - cosine(embed1,embed2)\n",
    "#         list_of_distances.append([text1, text2, cos_dist])\n",
    "\n",
    "\n",
    "# distances_df = pd.DataFrame(list_of_distances, columns = ['text1','text2','distance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d31ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"finalData.csv\")\n",
    "import pandas as pd\n",
    "df = pd.read_csv('collected_labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92d7bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "048ef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('collected_labeled_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91654b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_df = df[df.label==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6649f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_df.to_csv(\"neutral.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff995c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X, test_X = train_test_split(df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "012c49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X.to_csv('train.csv',index = False)\n",
    "# test_X.to_csv('test.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6103b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_len(text):\n",
    "#     txt = text.split(' ')[:20]\n",
    "#     return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0aeeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(check_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57168ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_word_embeddings(text):\n",
    "#     tokenizer = tokenizers\n",
    "#     tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "#     list_token_embeddings = get_bert_embeddings(tokens_tensors, segments_tensors, model)\n",
    "#     ## list_token_embeddings has embeddings of the given words\n",
    "#     return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36144615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords= stopwords.words(\"nepali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2163997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = ['अक्सर','आदि','कसरी','अन्तर्गत','अर्थात','अर्थात्','अलग','आयो','उदाहरण','एकदम','राम्रो','बिरुद्ध','बिशेष','नराम्रो']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67268e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = list(set(stopwords).difference(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9ea0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_emojis(text):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#         u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "#         u\"\\U00002702-\\U000027B0\"\n",
    "#         u\"\\U00002702-\\U000027B0\"\n",
    "#         u\"\\U000024C2-\\U0001F251\"\n",
    "#         u\"\\U0001f926-\\U0001f937\"\n",
    "#         u\"\\U00010000-\\U0010ffff\"\n",
    "#         u\"\\u2640-\\u2642\" \n",
    "#         u\"\\u2600-\\u2B55\"\n",
    "#         u\"\\u200d\"\n",
    "#         u\"\\u23cf\"\n",
    "#         u\"\\u23e9\"\n",
    "#         u\"\\u231a\"\n",
    "#         u\"\\ufe0f\" # dingbats\n",
    "#         u\"\\u3030\"\n",
    "#     \"]+\", re.UNICODE)\n",
    "#     text = emoji_pattern.sub(r'', text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af7b34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     text = remove_emojis(text)\n",
    "# #     text = text.split(' ')\n",
    "# #     clean_text_list = []\n",
    "# #     for word in text:\n",
    "# #         if word not in stopwords:\n",
    "# #             clean_text_list.append(word)\n",
    "# #     clean_text = ' '.join(clean_text_list)\n",
    "# #     stem_words = nepali_stemmer.stemWords(text.split())\n",
    "# #     stem_text = ' '.join(stem_words)\n",
    "# #     txt = re.sub(r\"[|a-zA-z.'#0-9@,:?'\\u200b\\u200c\\u200d!/&~-]\",'',text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05ec9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text(\"घाम जति लग्यो हामीलाई तेती राम्रो हुन्छ apple \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61c1b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3b2275f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>बजार ले जसरी ट्रेन्ड चेन्ज गर्यो यो हेर्दा तत्...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000 अंकले घटेको नेप्से 200 अंकले बढ्नु ठूलो क...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>होइन यो सानिमा बैंक ले bonus घोसणा गरेको २ महि...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>खैँ MBJC प्रति कित्तामा रू,10/-ले बढेर आज रू,1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>राम्रो भयो️️</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  बजार ले जसरी ट्रेन्ड चेन्ज गर्यो यो हेर्दा तत्...      2\n",
       "1  1000 अंकले घटेको नेप्से 200 अंकले बढ्नु ठूलो क...      1\n",
       "2  होइन यो सानिमा बैंक ले bonus घोसणा गरेको २ महि...      2\n",
       "3  खैँ MBJC प्रति कित्तामा रू,10/-ले बढेर आज रू,1...      2\n",
       "4                                       राम्रो भयो️️      1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b76b3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding_sentence(input_sentence):\n",
    "#     md = model\n",
    "#     tokenizer = tokenizers\n",
    "    md = local_model\n",
    "    tokenizer = local_tokenizers\n",
    "    marked_text = \" [CLS] \" + input_sentence + \" [SEP] \"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(indexed_tokens) \n",
    "    \n",
    "    # Convert inputs to Pytorch tensors\n",
    "    tokens_tensors = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = md(tokens_tensors, segments_tensors)\n",
    "        # removing the first hidden state\n",
    "        # the first state is the input state \n",
    "\n",
    "        hidden_states = outputs.hidden_states\n",
    "#         print(hidden_states[-2])\n",
    "        # second_hidden_states = outputs[2]\n",
    "    # hidden_states has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "    # token_vecs is a tensor with shape [22 x 768]\n",
    "#     token_vecs = hidden_states[-2][0]\n",
    "    # get last four layers\n",
    "#     last_four_layers = [hidden_states[i] for i in (-1,-2, -3,-4)]\n",
    "\n",
    "\n",
    "    # cast layers to a tuple and concatenate over the last dimension\n",
    "#     cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "#     print(cat_hidden_states.shape)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # take the mean of the concatenated vector over the token dimension\n",
    "#     sentence_embedding = torch.mean(cat_hidden_states, dim=0).squeeze()\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "#     sentence_embedding = torch.mean(token_vecs, dim=1)\n",
    "    return sentence_embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1da99701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_bert_embedding_sentence(\"नेपाल को ससकृती ध्वस्त पार्ने योजना\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d08f787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df[df['label']==2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c8990f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba7e75a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35466/4042849988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_bert_embedding_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                     \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 )\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35466/1958932652.py\u001b[0m in \u001b[0;36mget_bert_embedding_sentence\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     md = model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     tokenizer = tokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_tokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmarked_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" [CLS] \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" [SEP] \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_model' is not defined"
     ]
    }
   ],
   "source": [
    "df['word_embeddings'] = df['text'].apply(get_bert_embedding_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edad3099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6056, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4760c1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000 अंकले घटेको नेप्से 200 अंकले बढ्नु ठूलो क...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.26195702, 0.53875995, -0.16427916, 0.25294...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>राम्रो भयो️️</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.30638546, 0.9532779, -0.38146943, 0.543288...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>जानकारीको लागि धन्यवाद रामहरी ब्रदर</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3177412, 0.658264, -0.12128312, 0.06808513,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>भारत-मधेस र नेपाल-चीन सम्बन्ध विग्रन्छ, मधेसी ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.15587394, 0.6471651, -0.13978052, -0.099823...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>लेखनाथ न्यौपानेको खुलासा,महाधिबेशनमा एमसीसीको ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.1403608, 0.88074046, 0.0538228, 0.11658574...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label  \\\n",
       "1   1000 अंकले घटेको नेप्से 200 अंकले बढ्नु ठूलो क...      1   \n",
       "4                                        राम्रो भयो️️      1   \n",
       "6                 जानकारीको लागि धन्यवाद रामहरी ब्रदर      1   \n",
       "18  भारत-मधेस र नेपाल-चीन सम्बन्ध विग्रन्छ, मधेसी ...      0   \n",
       "25  लेखनाथ न्यौपानेको खुलासा,महाधिबेशनमा एमसीसीको ...      0   \n",
       "\n",
       "                                      word_embeddings  \n",
       "1   [-0.26195702, 0.53875995, -0.16427916, 0.25294...  \n",
       "4   [-0.30638546, 0.9532779, -0.38146943, 0.543288...  \n",
       "6   [0.3177412, 0.658264, -0.12128312, 0.06808513,...  \n",
       "18  [0.15587394, 0.6471651, -0.13978052, -0.099823...  \n",
       "25  [-0.1403608, 0.88074046, 0.0538228, 0.11658574...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc3840ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('embedding_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2da7b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df['word_embeddings'], df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6bc72bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# pca = PCA(n_components = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99ad87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_X = scaler.fit_transform(X.tolist())\n",
    "# pca_X = pca.fit_transform(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9689b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y, test_size = 0.2, random_state = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5af8677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "828e1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6524c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = [i[0] for i in train_X]\n",
    "# test_X = [i[0] for i in test_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8311883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2af91c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(train_X.tolist(), train_y)\n",
    "#svc.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16d5e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pred = svc.predict(test_X.tolist())\n",
    "# svc_pred = svc.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdd814fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[429  86]\n",
      " [ 67 630]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_y, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c87a1d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85       515\n",
      "           1       0.88      0.90      0.89       697\n",
      "\n",
      "    accuracy                           0.87      1212\n",
      "   macro avg       0.87      0.87      0.87      1212\n",
      "weighted avg       0.87      0.87      0.87      1212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78fe89bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8737623762376238"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87c34455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89171974522293"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa889bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नराम्रो is negative sentiment\n"
     ]
    }
   ],
   "source": [
    "sent = \"नराम्रो\"\n",
    "predicted_label = svc.predict_proba(np.array(get_bert_embedding_sentence(sent).tolist()).reshape(1,-1))[0]\n",
    "if predicted_label[0]<predicted_label[1]:\n",
    "    print(f'{sent} is negative sentiment')\n",
    "else:\n",
    "    print(f'{sent} is positive sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c5d51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svc, open('scv_sentiment','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00640092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pickle.dump(svc,open('svc_sentiment','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cdc460bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/info/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svc_sentiment = pickle.load(open('svc_sentiment','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0791ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46629947, 0.53370053])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b884630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
