{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1d8adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f08f3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"covid19_tweeter_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c822dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tokanize_tweet</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 22:06:41+00:00</td>\n",
       "      <td>अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...</td>\n",
       "      <td>अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 17:49:34+00:00</td>\n",
       "      <td>कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...</td>\n",
       "      <td>कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-10 16:18:34+00:00</td>\n",
       "      <td>नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...</td>\n",
       "      <td>नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-10 15:12:17+00:00</td>\n",
       "      <td>कोभिड को खोप पनि लगाइयो</td>\n",
       "      <td>कोभिड,को,खोप,पनि,लगाइयो</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 15:07:12+00:00</td>\n",
       "      <td>अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...</td>\n",
       "      <td>अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33469</th>\n",
       "      <td>33469</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-12 01:33:08+00:00</td>\n",
       "      <td>कोरोना भाइरसलाई विश्व स्वास्थ्य संगठनले दियो न...</td>\n",
       "      <td>कोरोना,भाइरसलाई,विश्व,स्वास्थ्य,संगठनले,नाम,कोभिड</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33470</th>\n",
       "      <td>33470</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-12 00:38:07+00:00</td>\n",
       "      <td>डब्ल्युएचओले दियाेको कोरोनाको नयाँ नाम कोभिड</td>\n",
       "      <td>डब्ल्युएचओले,दियाेको,कोरोनाको,नाम,कोभिड</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33471</th>\n",
       "      <td>33471</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-11 19:37:26+00:00</td>\n",
       "      <td>कोरोना भाइरस संक्रमणको औपचारिक नाम अब कोभिड</td>\n",
       "      <td>कोरोना,भाइरस,संक्रमणको,औपचारिक,नाम,कोभिड</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33472</th>\n",
       "      <td>33472</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-11 16:36:40+00:00</td>\n",
       "      <td>विश्व स्वास्थ्य संगठन डब्लुएचओ ले नोबल कोरोना ...</td>\n",
       "      <td>विश्व,स्वास्थ्य,संगठन,डब्लुएचओ,नोबल,कोरोना,भाइ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33473</th>\n",
       "      <td>33473</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-11 15:22:16+00:00</td>\n",
       "      <td>कोरोनाभाइरसका सङ्क्रमणको नाम कोभिड भनेर विश्व ...</td>\n",
       "      <td>कोरोनाभाइरसका,सङ्क्रमणको,नाम,कोभिड,विश्व,स्वास...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33474 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Label                   Datetime  \\\n",
       "0               0     -1  2021-01-10 22:06:41+00:00   \n",
       "1               1     -1  2021-01-10 17:49:34+00:00   \n",
       "2               2      1  2021-01-10 16:18:34+00:00   \n",
       "3               3      0  2021-01-10 15:12:17+00:00   \n",
       "4               4     -1  2021-01-10 15:07:12+00:00   \n",
       "...           ...    ...                        ...   \n",
       "33469       33469      0  2020-02-12 01:33:08+00:00   \n",
       "33470       33470      0  2020-02-12 00:38:07+00:00   \n",
       "33471       33471      0  2020-02-11 19:37:26+00:00   \n",
       "33472       33472      0  2020-02-11 16:36:40+00:00   \n",
       "33473       33473      0  2020-02-11 15:22:16+00:00   \n",
       "\n",
       "                                                   Tweet  \\\n",
       "0      अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...   \n",
       "1      कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...   \n",
       "2      नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...   \n",
       "3                                कोभिड को खोप पनि लगाइयो   \n",
       "4      अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...   \n",
       "...                                                  ...   \n",
       "33469  कोरोना भाइरसलाई विश्व स्वास्थ्य संगठनले दियो न...   \n",
       "33470       डब्ल्युएचओले दियाेको कोरोनाको नयाँ नाम कोभिड   \n",
       "33471        कोरोना भाइरस संक्रमणको औपचारिक नाम अब कोभिड   \n",
       "33472  विश्व स्वास्थ्य संगठन डब्लुएचओ ले नोबल कोरोना ...   \n",
       "33473  कोरोनाभाइरसका सङ्क्रमणको नाम कोभिड भनेर विश्व ...   \n",
       "\n",
       "                                          Tokanize_tweet Unnamed: 5  \n",
       "0      अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...        NaN  \n",
       "1      कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...        NaN  \n",
       "2      नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...        NaN  \n",
       "3                                कोभिड,को,खोप,पनि,लगाइयो        NaN  \n",
       "4      अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...        NaN  \n",
       "...                                                  ...        ...  \n",
       "33469  कोरोना,भाइरसलाई,विश्व,स्वास्थ्य,संगठनले,नाम,कोभिड        NaN  \n",
       "33470            डब्ल्युएचओले,दियाेको,कोरोनाको,नाम,कोभिड        NaN  \n",
       "33471           कोरोना,भाइरस,संक्रमणको,औपचारिक,नाम,कोभिड        NaN  \n",
       "33472  विश्व,स्वास्थ्य,संगठन,डब्लुएचओ,नोबल,कोरोना,भाइ...        NaN  \n",
       "33473  कोरोनाभाइरसका,सङ्क्रमणको,नाम,कोभिड,विश्व,स्वास...        NaN  \n",
       "\n",
       "[33474 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2411ec71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'प्रधानमँत्रीज्यू चुनाव गर्ने पैसा हुने तर कोभीड बाट जनताको लागि निशूल्क खोप ल्याउन पैसा नहुने खोप ल्याउन किन ढिलाइ गरिँदैछ देश र जनताप्रति किन लापरबाही'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d04891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'] = data['Tokanize_tweet'].apply(\n",
    "    lambda row: min(len(row.split(\",\")), len(row)) if isinstance(row, str) else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8922dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data['length'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed98d659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक स्वास्थ्य सम्बन्धि समस्या देखिएको र आत्महत्याका घटनाहरु बढेको डा सापकोटाले बताए'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Tweet\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "177b8fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tokanize_tweet</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 22:06:41+00:00</td>\n",
       "      <td>अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...</td>\n",
       "      <td>अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 17:49:34+00:00</td>\n",
       "      <td>कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...</td>\n",
       "      <td>कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-10 16:18:34+00:00</td>\n",
       "      <td>नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...</td>\n",
       "      <td>नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-10 15:12:17+00:00</td>\n",
       "      <td>कोभिड को खोप पनि लगाइयो</td>\n",
       "      <td>कोभिड,को,खोप,पनि,लगाइयो</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 15:07:12+00:00</td>\n",
       "      <td>अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...</td>\n",
       "      <td>अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Label                   Datetime  \\\n",
       "0           0     -1  2021-01-10 22:06:41+00:00   \n",
       "1           1     -1  2021-01-10 17:49:34+00:00   \n",
       "2           2      1  2021-01-10 16:18:34+00:00   \n",
       "3           3      0  2021-01-10 15:12:17+00:00   \n",
       "4           4     -1  2021-01-10 15:07:12+00:00   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...   \n",
       "1  कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...   \n",
       "2  नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...   \n",
       "3                            कोभिड को खोप पनि लगाइयो   \n",
       "4  अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...   \n",
       "\n",
       "                                      Tokanize_tweet Unnamed: 5  length  \n",
       "0  अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...        NaN     9.0  \n",
       "1  कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...        NaN    18.0  \n",
       "2  नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...        NaN    20.0  \n",
       "3                            कोभिड,को,खोप,पनि,लगाइयो        NaN     5.0  \n",
       "4  अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...        NaN    12.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2464feae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक,स्वास्थ्य,सम्बन्धि,समस्या,देखिएको,र,आत्महत्याका,घटनाहरु,बढेको,डा,सापकोटाले,बताए'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Tokanize_tweet\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18969988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    14998\n",
       "-1    13606\n",
       " 0     4870\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ce0c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n",
    "from scipy.spatial.distance import cosine \n",
    "import tokenizers \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import snowballstemmer\n",
    "import numpy\n",
    "import os \n",
    "import re\n",
    "import json\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b519b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Shushant/nepaliBERT/resolve/main/config.json from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/c642996ea6b3b6351e3519c0b5059b07a6b8a1a8e4961bf81d181326b75cabfe.3abddfe906646a59813d34e11ed576b8ddb2023553640fa3ce71f000d3c234d0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"Shushant/nepaliBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Shushant/nepaliBERT/resolve/main/pytorch_model.bin from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/2d09fc338e59d3f5d3e93e545c8eecd8fa75ca9ac28c3df0a4a8757717a91951.12dd15b86f6dfca7012f1ae623f9c1770e16a05169b9a668a028226aa4477824\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at Shushant/nepaliBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"Shushant/nepaliBERT\", output_hidden_states = True, return_dict = True, output_attentions = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7dc414c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/Shushant/nepaliBERT/resolve/main/config.json from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/c642996ea6b3b6351e3519c0b5059b07a6b8a1a8e4961bf81d181326b75cabfe.3abddfe906646a59813d34e11ed576b8ddb2023553640fa3ce71f000d3c234d0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"Shushant/nepaliBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/vocab.txt from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/22c9a4d244e3f42334b0f06c2faa3df5130e6b533c42be86c1e3ed28ba385eb4.a31f6709bd9b70ccff6b17176e4c942c6860ae0b36de53bafc7ab6a74ab3e735\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/Shushant/nepaliBERT/resolve/main/config.json from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/c642996ea6b3b6351e3519c0b5059b07a6b8a1a8e4961bf81d181326b75cabfe.3abddfe906646a59813d34e11ed576b8ddb2023553640fa3ce71f000d3c234d0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"Shushant/nepaliBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/Shushant/nepaliBERT/resolve/main/config.json from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/c642996ea6b3b6351e3519c0b5059b07a6b8a1a8e4961bf81d181326b75cabfe.3abddfe906646a59813d34e11ed576b8ddb2023553640fa3ce71f000d3c234d0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"Shushant/nepaliBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizers = AutoTokenizer.from_pretrained(\"Shushant/nepaliBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bd3f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('bert_embeddings','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ac250ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizers, open('bert_tokenizers','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b1e9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('bert_embeddings','rb'))\n",
    "tokenizers= pickle.load(open('bert_tokenizers','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1871cd20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['क',\n",
       " 'मौ',\n",
       " '##ज',\n",
       " '##दा',\n",
       " 'लोक',\n",
       " '##तान',\n",
       " '##तर',\n",
       " '##िक',\n",
       " 'वय',\n",
       " '##वस',\n",
       " '##था',\n",
       " 'राज',\n",
       " '##य',\n",
       " 'पन',\n",
       " '##ः',\n",
       " '##सर',\n",
       " '##चना',\n",
       " '##सग',\n",
       " 'जोडिएका',\n",
       " 'हिजोका',\n",
       " 'सवाल',\n",
       " '##हर',\n",
       " '##लाई',\n",
       " 'यथा',\n",
       " '##स',\n",
       " '##थिति',\n",
       " '##मा',\n",
       " 'छोड',\n",
       " '##र',\n",
       " 'सबल',\n",
       " 'होला',\n",
       " '?']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.tokenize(\"के मौजुदा लोकतान्त्रिक व्यवस्था राज्य पुनःसंरचनासँग जोडिएका हिजोका सवालहरूलाई यथास्थितिमा छोडेर सबल होला?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08820a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding_sentence(input_sentence):\n",
    "    md = model\n",
    "    tokenizer = tokenizers\n",
    "#     md = local_model\n",
    "#     tokenizer = local_tokenizers\n",
    "    marked_text = \" [CLS] \" + input_sentence + \" [SEP] \"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(indexed_tokens) \n",
    "    \n",
    "    # Convert inputs to Pytorch tensors\n",
    "    tokens_tensors = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = md(tokens_tensors, segments_tensors)\n",
    "        # removing the first hidden state\n",
    "        # the first state is the input state \n",
    "\n",
    "        hidden_states = outputs.hidden_states\n",
    "#         print(hidden_states[-2])\n",
    "        # second_hidden_states = outputs[2]\n",
    "    # hidden_states has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "    # token_vecs is a tensor with shape [22 x 768]\n",
    "#     token_vecs = hidden_states[-2][0]\n",
    "    # get last four layers\n",
    "#     last_four_layers = [hidden_states[i] for i in (-1,-2, -3,-4)]\n",
    "    # cast layers to a tuple and concatenate over the last dimension\n",
    "#     cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "#     print(cat_hidden_states.shape)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    # take the mean of the concatenated vector over the token dimension\n",
    "#     sentence_embedding = torch.mean(cat_hidden_states, dim=0).squeeze()\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "#     sentence_embedding = torch.mean(token_vecs, dim=1)\n",
    "    return sentence_embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a1a2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_embeddings'] = data['Tweet'].apply(get_bert_embedding_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16889493",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data['word_embeddings'], data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fad91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y, test_size = 0.2, random_state = 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a560a",
   "metadata": {},
   "source": [
    "train_df,test_df = train_test_split(data, test_size = 0.2, random_state = 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4a917",
   "metadata": {},
   "source": [
    "train_df.to_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbff085",
   "metadata": {},
   "source": [
    "test_df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f706ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba420c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(train_X.tolist(), train_y)\n",
    "#svc.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a4c4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pred = svc.predict(test_X.tolist())\n",
    "# svc_pred = svc.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf9e1f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2258   49  449]\n",
      " [ 312  188  484]\n",
      " [ 505   48 2402]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_y, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "614ae1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.82      0.77      2756\n",
      "           0       0.66      0.19      0.30       984\n",
      "           1       0.72      0.81      0.76      2955\n",
      "\n",
      "    accuracy                           0.72      6695\n",
      "   macro avg       0.70      0.61      0.61      6695\n",
      "weighted avg       0.72      0.72      0.70      6695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4359624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7241224794622853"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16ae2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नराम्रो is negative sentiment\n"
     ]
    }
   ],
   "source": [
    "sent = \"नराम्रो\"\n",
    "predicted_label = svc.predict_proba(np.array(get_bert_embedding_sentence(sent).tolist()).reshape(1,-1))[0]\n",
    "if predicted_label[0]<predicted_label[1]:\n",
    "    print(f'{sent} is negative sentiment')\n",
    "else:\n",
    "    print(f'{sent} is positive sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5835169a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6766243465272591"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(train_X.tolist(), train_y)\n",
    "gbc.score(test_X.tolist(), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5270babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fm-pc-lt-228/anaconda3/envs/nepali/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7047050037341299"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(train_X.tolist(), train_y)\n",
    "\n",
    "\n",
    "\n",
    "clf.score(test_X.tolist(), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a5e954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5870052277819268"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_X.tolist(), train_y)\n",
    "gnb.score(test_X.tolist(), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6c4381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1491  599  666]\n",
      " [ 124  473  387]\n",
      " [ 336  653 1966]]\n"
     ]
    }
   ],
   "source": [
    "gnb_pred = gnb.predict(test_X.tolist())\n",
    "print(confusion_matrix(test_y, gnb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957caa2c",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b3b7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4a203",
   "metadata": {},
   "source": [
    "## Finetuning with NepBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "706da9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Shushant/nepaliBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Shushant/nepaliBERT and are newly initialized: ['classifier.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Shushant/nepaliBERT\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab57932f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-17 22:53:52.854163: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-17 22:53:52.878462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-17 22:53:52.878476: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6ce2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ad2080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Shushant--CovidNepaliTweets-00a71fabf99a7743\n",
      "Reusing dataset csv (/home/fm-pc-lt-228/.cache/huggingface/datasets/Shushant___csv/Shushant--CovidNepaliTweets-00a71fabf99a7743/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015757083892822266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 59,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9180d10665424da4278d5bd7ee7601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Label': 0,\n",
       " 'Text': 'नेपाली कांग्रेसका वरिष्ठनेता अादरणिय श्री रामचन्द्र पौडेलज्यूको प्रमुख अातिथ्यतामा कोभिड पछिको विश्वसन्दर्भमा लोकतान्त्रिक समाजबादको सान्दर्भिकता विषयक प्रदेश स्तरिय जुम मिटिङ्ग फेसबुकमा गते जेष्ठ बुधबार बजे लाईभ गरिनेछ सामाजिक लोकतन्त्र अध्ययन केन्द्र'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Shushant/CovidNepaliTweets\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7904c32d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/Shushant/nepaliBERT/resolve/main/config.json from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/c642996ea6b3b6351e3519c0b5059b07a6b8a1a8e4961bf81d181326b75cabfe.3abddfe906646a59813d34e11ed576b8ddb2023553640fa3ce71f000d3c234d0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"Shushant/nepaliBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/vocab.txt from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/22c9a4d244e3f42334b0f06c2faa3df5130e6b533c42be86c1e3ed28ba385eb4.a31f6709bd9b70ccff6b17176e4c942c6860ae0b36de53bafc7ab6a74ab3e735\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/Shushant/nepaliBERT/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/Shushant/nepaliBERT/resolve/main/config.json from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/c642996ea6b3b6351e3519c0b5059b07a6b8a1a8e4961bf81d181326b75cabfe.3abddfe906646a59813d34e11ed576b8ddb2023553640fa3ce71f000d3c234d0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"Shushant/nepaliBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/Shushant/nepaliBERT/resolve/main/config.json from cache at /home/fm-pc-lt-228/.cache/huggingface/transformers/c642996ea6b3b6351e3519c0b5059b07a6b8a1a8e4961bf81d181326b75cabfe.3abddfe906646a59813d34e11ed576b8ddb2023553640fa3ce71f000d3c234d0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"Shushant/nepaliBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00965118408203125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 59,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 27,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f418a73fd3e74470b3af3f4f4434929f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009561538696289062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 59,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d74ea07c7249be96d3b2879252f14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Shushant/nepaliBERT\")\n",
    "\n",
    "max_length = 61\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'], padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7590bae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Label': 1,\n",
       " 'Text': 'आचार्य जी हर बिमारी का इलाज ढुढलेते हैं आप अब तक कोभीड का कोइ इलाज वा दवा क्यों नहिं खोज पाए हैं',\n",
       " 'input_ids': [2,\n",
       "  8575,\n",
       "  1022,\n",
       "  2260,\n",
       "  2699,\n",
       "  7590,\n",
       "  1714,\n",
       "  1701,\n",
       "  9525,\n",
       "  1037,\n",
       "  380,\n",
       "  1061,\n",
       "  4102,\n",
       "  402,\n",
       "  4167,\n",
       "  2227,\n",
       "  30176,\n",
       "  1805,\n",
       "  4690,\n",
       "  1051,\n",
       "  1701,\n",
       "  3615,\n",
       "  9525,\n",
       "  1037,\n",
       "  1862,\n",
       "  8773,\n",
       "  367,\n",
       "  1700,\n",
       "  11992,\n",
       "  1027,\n",
       "  2426,\n",
       "  4583,\n",
       "  402,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d20dd332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets['train'][0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b960bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb0d3d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50154ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "496ce1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Label, Text.\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_121287/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m         )\n\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;31m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_extended_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;31m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nepali/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mget_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Since we are adding it to the raw scores before the softmax, this is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# effectively the same as removing these entirely.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fp16 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734302a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
